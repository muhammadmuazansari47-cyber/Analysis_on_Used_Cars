{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Big Data Setup ---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import os\n",
    "\n",
    "# Spark Session shuru karna\n",
    "spark = SparkSession.builder.appName(\"Vehicles_PySpark_Cleaning\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Output folders banana\n",
    "os.makedirs(\"data_raw\", exist_ok=True)\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "\n",
    "print(\" Spark Session ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Load & Initial Schema ---\n",
    "# 'vehicles.csv' file ko distributed DataFrame mein load karna\n",
    "# Assuming your file is in 'data_raw/vehicles.csv'\n",
    "path =\"/content/vehicles.csv\"\n",
    "df_raw = spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Total rows loaded: {df_raw.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Irrelevant Columns Drop Karna ---\n",
    "columns_to_drop = ['id','url','region_url','VIN','image_url','description','state', 'lat', 'long','posting_date', 'county']\n",
    "\n",
    "df_clean = df_raw.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dropping Columns with >25% Nulls ---\n",
    "total_count = df_clean.count()\n",
    "threshold = 0.25\n",
    "\n",
    "# Null percentage nikalna aur columns drop karna\n",
    "null_counts = [(col, df_clean.filter(F.col(col).isNull()).count()) for col in df_clean.columns]\n",
    "cols_to_drop_25p = [col for col, count in null_counts if (count / total_count) > threshold]\n",
    "\n",
    "# Purane code ke mutabik yeh 'size', 'cylinders', 'condition' honge.\n",
    "df_clean = df_clean.drop(*cols_to_drop_25p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a939f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dropping Rows with Nulls in Key Columns ---\n",
    "# Key visualization aur analysis ke liye zaroori rows ko drop karna\n",
    "df_clean = df_clean.na.drop(subset=['price', 'odometer', 'type', 'title_status', 'year', 'transmission', 'manufacturer', 'model', 'fuel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe28999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Type Casting (Data types ko theek karna) ---\n",
    "df_clean = df_clean.withColumn(\"year\", F.col(\"year\").cast(IntegerType()))\n",
    "df_clean = df_clean.withColumn(\"price\", F.col(\"price\").cast(DoubleType()))\n",
    "df_clean = df_clean.withColumn(\"odometer\", F.col(\"odometer\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ad6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Outlier Handling (Price) ---\n",
    "# (249999 se zyada aur 100 se kam hataana, jaisa ki pehle kiya tha)\n",
    "df_clean = df_clean.filter((F.col(\"price\") > 100) & (F.col(\"price\") < 249999))\n",
    "print(f\"Rows after all cleaning steps: {df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Visualization Data Save Karna (TEXT & NUMERIC) ---\n",
    "\n",
    "df_viz_final = df_clean.select(\n",
    "    \"price\",\n",
    "    \"odometer\",\n",
    "    \"year\",\n",
    "    \"manufacturer\",\n",
    "    \"model\",\n",
    "    \"type\",\n",
    "    \"transmission\",\n",
    "    \"title_status\",\n",
    "    \"fuel\"\n",
    ")\n",
    "\n",
    "# CSV mein save karna. 'coalesce(1)' zaroori hai taaki Spark ek single output file banaye.\n",
    "df_viz_final.coalesce(1).write.csv(\n",
    "    \"craigslist_data.csv\",\n",
    "mode=\"overwrite\",\n",
    "header=(True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"\\n Overall Linear Regression: Depreciation Rate Calculation (PySpark MLlib)\")\n",
    "\n",
    "# 1. Outlier Filtering (PySpark equivalent of Z-score < 3)\n",
    "# Mean aur Standard Deviation calculate karna\n",
    "stats_df = df_clean.agg(\n",
    "    F.mean(\"price\").alias(\"price_mean\"),\n",
    "    F.stddev(\"price\").alias(\"price_std\"),\n",
    "    F.mean(\"odometer\").alias(\"odometer_mean\"),\n",
    "    F.stddev(\"odometer\").alias(\"odometer_std\")\n",
    ").collect()[0]\n",
    "\n",
    "# Filtering Conditions banana (3 standard deviations ke andar)\n",
    "df_depr_filtered = df_clean.filter(\n",
    "    (F.col(\"price\") > stats_df[\"price_mean\"] - 3 * stats_df[\"price_std\"]) &\n",
    "    (F.col(\"price\") < stats_df[\"price_mean\"] + 3 * stats_df[\"price_std\"]) &\n",
    "    (F.col(\"odometer\") > stats_df[\"odometer_mean\"] - 3 * stats_df[\"odometer_std\"]) &\n",
    "    (F.col(\"odometer\") < stats_df[\"odometer_mean\"] + 3 * stats_df[\"odometer_std\"])\n",
    ").select(\"price\", \"odometer\")\n",
    "\n",
    "print(f\"Rows after 3-sigma outlier filtering: {df_depr_filtered.count():,}\")\n",
    "\n",
    "# 2. Vector Assembler: PySpark ML ke liye features ko vector mein jodna\n",
    "assembler = VectorAssembler(inputCols=['odometer'], outputCol=\"features\")\n",
    "df_lr_ready = assembler.transform(df_depr_filtered)\n",
    "\n",
    "# 3. Distributed Linear Regression Model Fit Karna (PySpark MLlib)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr_model = lr.fit(df_lr_ready)\n",
    "\n",
    "# 4. Result Nikalna: Coefficient (Depreciation Rate)\n",
    "depreciation_per_mile = lr_model.coefficients[0]\n",
    "loss_per_10k_miles = depreciation_per_mile * 10000\n",
    "\n",
    "# Final Results Print Karna\n",
    "print(f\"\\n Odometer Coefficient (Depreciation Rate per 1 Mile): ${depreciation_per_mile:,.4f}\")\n",
    "print(f\" Loss Per 10,000 Miles: ${loss_per_10k_miles:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Grouped Map Apply Karna (Distributed Analysis)\n",
    "master_depreciation_results_spark = df_clean.groupby(\"manufacturer\", \"model\").apply(calculate_depreciation_udf)\n",
    "\n",
    "# 4. Final Cleanup aur Save Karna (Master Table)\n",
    "# Depreciation Rate ko positive loss mein badalna\n",
    "master_depreciation_results_spark = master_depreciation_results_spark.withColumn(\n",
    "    'Est_Loss_Per_10k_Miles_USD', F.col('Loss_Per_10k_Miles') * -1)\n",
    "master_depreciation_results_spark = master_depreciation_results_spark.withColumn(\n",
    "    'Model_Reliability_R2', F.round(F.col('R2_Score'), 3))\n",
    "\n",
    "\n",
    "# Final columns chunna aur save karna (jaisa ki aapke original code mein tha)\n",
    "df_final_master = master_depreciation_results_spark.select(\n",
    "    \"manufacturer\",\n",
    "    \"model\",\n",
    "    \"Model_Count\",\n",
    "    \"Est_Loss_Per_10k_Miles_USD\",\n",
    "    \"Model_Reliability_R2\"\n",
    ")\n",
    "\n",
    "df_final_master.coalesce(1).write.csv(\n",
    "    \"reports/MASTER_DEPRECIATION_RATES.csv\",\n",
    "    mode=\"overwrite\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\" MASTER_DEPRECIATION_RATES.csv file successfully created (Power BI Master Table).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Overall Depreciation per 1 Mile', 'Overall Loss Per 10k Miles'],\n",
    "    'Value': [depreciation_per_mile, loss_per_10k_miles],\n",
    "    'Unit': ['USD', 'USD']\n",
    "})\n",
    "\n",
    "results_df.to_csv('reports/analysis_key_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
