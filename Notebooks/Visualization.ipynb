{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Vehicles_PySpark_Analysis\").getOrCreate()\n",
    "\n",
    "# Cleaned data ko load karna (CSV se ya yahan dobara cleaning karne ke baad)\n",
    "# Is analysis ke liye hum clean data ko dobara load karte hain\n",
    "df_clean = spark.read.csv(\"/content/craigslist_data_for_power_bi_FINAL_TEXT_2.csv\", header=True, inferSchema=True)\n",
    "print(f\"Analysis data loaded: {df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dedd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# --- Feature Engineering: Age Calculation and Cleaning ---\n",
    "\n",
    "# 1. Age Calculate Karna (PySpark)\n",
    "# Humne pichle steps mein 2025 use kiya tha, wahi rakh rahe hain.\n",
    "df_clean = df_clean.withColumn('Age', F.lit(2025).cast(IntegerType()) - F.col('year').cast(IntegerType()))\n",
    "\n",
    "# 2. Impossible Age Values (Negative/Zero) Filter Karna (PySpark)\n",
    "# Agar koi galti se future year (e.g., 2030) daal de, toh Age negative ho jaayegi. Use filter karna.\n",
    "df_clean = df_clean.filter(F.col('Age') >= 0)\n",
    "\n",
    "print(\" Age calculation complete and impossible age values filtered.\")\n",
    "print(f\"Rows remaining after age check: {df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1211bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pandas UDF Schema Definition ---\n",
    "schema = StructType([\n",
    "    StructField(\"manufacturer\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"Model_Count\", LongType(), True),\n",
    "    StructField(\"Est_Loss_Per_10k_Miles_USD\", DoubleType(), True),\n",
    "    StructField(\"Model_Reliability_R2\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# --- Pandas UDF Definition (Model Run per Group) ---\n",
    "@F.pandas_udf(schema, functionType=F.PandasUDFType.GROUPED_MAP)\n",
    "def calculate_depreciation_udf(pandas_df):\n",
    "    if len(pandas_df) < 50:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Statsmodels OLS (Pandas mein chalta hai, lekin Spark framework ke andar)\n",
    "    try:\n",
    "        X = sm.add_constant(pandas_df['odometer'])\n",
    "        y = pandas_df['price']\n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "        depreciation_rate = model.params['odometer']\n",
    "\n",
    "        result = pd.DataFrame({\n",
    "            'manufacturer': [pandas_df['manufacturer'].iloc[0]],\n",
    "            'model': [pandas_df['model'].iloc[0]],\n",
    "            'Model_Count': [len(pandas_df)],\n",
    "            'Est_Loss_Per_10k_Miles_USD': [depreciation_rate * 10000 * -1], # Positive Loss\n",
    "            'Model_Reliability_R2': [model.rsquared]\n",
    "        })\n",
    "        return result\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Grouped Map Apply Karna (Distributed Analysis) ---\n",
    "master_depreciation_results_spark = df_clean.groupby(\"manufacturer\", \"model\").apply(calculate_depreciation_udf)\n",
    "\n",
    "# --- Final Save for Power BI ---\n",
    "master_depreciation_results_spark.coalesce(1).write.csv(\n",
    "    \"reports/MASTER_DEPRECIATION_RATES.csv\",\n",
    "    mode=\"overwrite\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"\\n MASTER_DEPRECIATION_RATES.csv file successfully created (Power BI Metrics Table).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73148f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall depreciation ke liye data ko Pandas mein le aao (ek baar mein)\n",
    "import numpy as np\n",
    "df_all_data = df_clean.select(\"price\", \"odometer\").toPandas()\n",
    "\n",
    "# --- CLEANING STEP (most important) ---\n",
    "df_all_data = df_all_data.replace([np.inf, -np.inf], np.nan)\n",
    "df_all_data = df_all_data.dropna(subset=['price', 'odometer'])\n",
    "\n",
    "# OLS Regression chalaana\n",
    "X = sm.add_constant(df_all_data['odometer'])\n",
    "y = df_all_data['price']\n",
    "\n",
    "model_overall = sm.OLS(y, X).fit()\n",
    "\n",
    "depreciation_per_mile = model_overall.params['odometer']\n",
    "loss_per_10k_miles = depreciation_per_mile * 10000 * -1\n",
    "\n",
    "# Final Pandas DataFrame bana kar save karna\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Overall_Loss_Per_10k_Miles'],\n",
    "    'Value': [loss_per_10k_miles],\n",
    "    'Unit': ['USD']\n",
    "})\n",
    "\n",
    "results_df.to_csv('reports/analysis_key_metrics.csv', index=False)\n",
    "print(\" analysis_key_metrics.csv saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Angle 1: Price vs. Odometer ---\n",
    "print(\" Visualizing Angle 1: Price vs. Odometer (Depreciation Trend)\")\n",
    "\n",
    "# PySpark se data ka 1% sample nikalna (tez plot ke liye)\n",
    "df_sample = df_clean.select(\"price\", \"odometer\").sample(False, 0.01, seed=42)\n",
    "df_viz = df_sample.toPandas() # PySpark to Pandas conversion\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(\n",
    "    data=df_viz,\n",
    "    x='odometer',\n",
    "    y='price',\n",
    "    scatter_kws={'alpha': 0.1, 's': 10}, # Scatter points ko halka karna\n",
    "    line_kws={'color': 'red', 'lw': 2}   # Regression line\n",
    ")\n",
    "\n",
    "plt.title('Price vs. Odometer Reading (Depreciation Trend)', fontsize=14)\n",
    "plt.xlabel('Odometer Reading (Miles)')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Angle 2: Car Type Distribution ---\n",
    "print(\"\\n Visualizing Angle 2: Top Car Types Ki Distribution\")\n",
    "\n",
    "# PySpark mein Aggregation\n",
    "df_type_counts = df_clean.groupBy('type').count()\n",
    "df_type_counts = df_type_counts.orderBy(F.desc('count')).limit(10) # Top 10 types\n",
    "\n",
    "# PySpark to Pandas for Plotting\n",
    "df_viz_type = df_type_counts.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='count',\n",
    "    y='type',\n",
    "    data=df_viz_type,\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "plt.title('Top 10 Car Types by Count', fontsize=14)\n",
    "plt.xlabel('Listing Count (Lakhs Mein)')\n",
    "plt.ylabel('Car Type (Original Text)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Angle 3: Average Price by Year ---\n",
    "print(\"\\n Visualizing Angle 3: Average Price by Year\")\n",
    "\n",
    "# PySpark mein Aggregation (Average Price nikalna)\n",
    "df_avg_price = df_clean.groupBy('year').agg(\n",
    "    F.round(F.mean('price'), 0).alias('Average_Price')\n",
    ")\n",
    "df_avg_price = df_avg_price.filter(F.col('year') > 1990) # Purani gaadiyon ko hatana\n",
    "df_avg_price = df_avg_price.orderBy('year')\n",
    "\n",
    "# PySpark to Pandas for Plotting\n",
    "df_viz_year = df_avg_price.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(\n",
    "    x='year',\n",
    "    y='Average_Price',\n",
    "    data=df_viz_year,\n",
    "    marker='o',\n",
    "    color='darkblue'\n",
    ")\n",
    "\n",
    "plt.title('Average Car Price Trend Over Time (1990+)', fontsize=14)\n",
    "plt.xlabel('Year of Manufacture')\n",
    "plt.ylabel('Average Price (USD)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Angle 4: Transmission Distribution ---\n",
    "print(\"\\n Visualizing Angle 4: Transmission Distribution\")\n",
    "\n",
    "# PySpark mein Aggregation\n",
    "df_trans_counts = df_clean.groupBy('transmission').count()\n",
    "\n",
    "# PySpark to Pandas for Plotting\n",
    "df_viz_trans = df_trans_counts.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x='transmission',\n",
    "    y='count',\n",
    "    data=df_viz_trans,\n",
    "    palette='pastel'\n",
    ")\n",
    "\n",
    "plt.title('Distribution by Transmission Type', fontsize=14)\n",
    "plt.xlabel('Transmission Type (Original Text)')\n",
    "plt.ylabel('Listing Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Angle 5: Top Manufacturers by Depreciation Loss ---\n",
    "print(\"\\n Visualizing Angle 5: Top 10 Manufacturers by Lowest Monetary Loss\")\n",
    "\n",
    "# 1. Depreciation Metrics file load karna (jo humne pichle step mein banayi thi)\n",
    "# Note: Yeh file choti hai, isliye hum ise Pandas se load kar rahe hain\n",
    "try:\n",
    "    df_depr_metrics = pd.read_csv('\\content\\reports\\MASTER_DEPRECIATION_RATES.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\" Error: MASTER_DEPRECIATION_RATES.csv file not found. Please ensure it was created and copied to /reports folder.\")\n",
    "    # Agar file nahi mili, toh aage nahi badhenge\n",
    "    # PySpark DataFrame ko Pandas mein convert karna (Grouped Data)\n",
    "    # (Agar aapne pichle step mein Spark DataFrame ko memory mein rakha ho toh)\n",
    "    # df_depr_metrics = master_depreciation_results_spark.toPandas()\n",
    "    # Uske bajaye, hum sirf code dikhayenge.\n",
    "    pass\n",
    "\n",
    "\n",
    "if 'df_depr_metrics' in locals() and not df_depr_metrics.empty:\n",
    "    # 2. Manufacturer level par average loss nikalna\n",
    "    df_manu_loss = df_depr_metrics.groupby('manufacturer')['Est_Loss_Per_10k_Miles_USD'].mean().reset_index()\n",
    "\n",
    "    # 3. Sabse kam depreciate hone waale (yani 'Loss' sabse kam ho) top 10 chuno\n",
    "    df_manu_loss_sorted = df_manu_loss.sort_values(by='Est_Loss_Per_10k_Miles_USD', ascending=True).head(10)\n",
    "\n",
    "    # 4. Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        x='Est_Loss_Per_10k_Miles_USD',\n",
    "        y='manufacturer',\n",
    "        data=df_manu_loss_sorted,\n",
    "        palette='magma_r' # Reverse palette, taaki Lowest Loss (Best) dark ho\n",
    "    )\n",
    "\n",
    "    plt.title('Top 10 Manufacturers: Lowest Estimated Loss per 10,000 Miles', fontsize=14)\n",
    "    plt.xlabel('Estimated Loss Per 10,000 Miles (USD)')\n",
    "    plt.ylabel('Manufacturer')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Is tarah se aapki visualization bhi complete ho jayegi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a5387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Visualization: Numerical Correlation Heatmap ---\n",
    "print(\"\\n Final Visualization: Numerical Correlation Heatmap\")\n",
    "\n",
    "# 1. Zaroori columns chuno aur PySpark to Pandas conversion (Yeh data already clean hai)\n",
    "# 'df_clean' aapka PySpark DataFrame hai\n",
    "df_corr_pandas = df_clean.select(\"price\", \"year\", \"odometer\").toPandas()\n",
    "\n",
    "# 2. Correlation Matrix calculate karna (Pandas ka istemal karke)\n",
    "correlation_matrix = df_corr_pandas[['price', 'year', 'odometer']].corr()\n",
    "\n",
    "# 3. Heatmap plot karna (Seaborn)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,  # Values ko display karna\n",
    "    cmap='coolwarm', # Color scheme\n",
    "    fmt=\".2f\",       # 2 decimal places tak\n",
    "    linewidths=.5,\n",
    "    linecolor='black'\n",
    ")\n",
    "\n",
    "plt.title('Correlation Heatmap: Price, Year, and Odometer', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447db367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Angle 6: Average Price by Age and Type (Depreciation Analysis) ---\n",
    "print(\"\\n Visualizing Angle 6: Average Price by Vehicle Age and Type\")\n",
    "\n",
    "# 1. Age Calculate Karna (Assuming current year is 2025 for consistency)\n",
    "# Note: 'df_clean' aapka PySpark DataFrame hai\n",
    "df_clean = df_clean.withColumn('Age', F.lit(2025) - F.col('year'))\n",
    "\n",
    "# 2. Age ko Categories mein Baantna (Pandas' pd.cut ki jagah PySpark's F.when)\n",
    "df_clean = df_clean.withColumn('Age_Group',\n",
    "    F.when(F.col('Age') <= 5, '0-5 Years (Newest)')\n",
    "     .when((F.col('Age') > 5) & (F.col('Age') <= 10), '6-10 Years (Mid-Age)')\n",
    "     .when((F.col('Age') > 10) & (F.col('Age') <= 20), '11-20 Years (Old)')\n",
    "     .otherwise('20+ Years (Very Old)')\n",
    ")\n",
    "\n",
    "# 3. Filtering & Average Price Calculate Karna (PySpark GroupBy)\n",
    "main_types = ['sedan', 'truck', 'suv', 'coupe', 'hatchback']\n",
    "\n",
    "avg_price_data_spark = df_clean.filter(F.col('type').isin(main_types)) \\\n",
    "                                .groupBy('Age_Group', 'type') \\\n",
    "                                .agg(F.mean('price').alias('Average_Price'))\n",
    "\n",
    "# 4. Visualization ke liye Pandas mein convert karna\n",
    "avg_price_data = avg_price_data_spark.toPandas()\n",
    "\n",
    "# 5. Age Grouping order theek karna taaki plot mein sahi sequence mein aaye\n",
    "age_order = ['0-5 Years (Newest)', '6-10 Years (Mid-Age)', '11-20 Years (Old)', '20+ Years (Very Old)']\n",
    "avg_price_data['Age_Group'] = pd.Categorical(avg_price_data['Age_Group'], categories=age_order, ordered=True)\n",
    "avg_price_data = avg_price_data.sort_values('Age_Group')\n",
    "\n",
    "\n",
    "# 6. Plotting (Seaborn)\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(\n",
    "    data=avg_price_data,\n",
    "    x='Age_Group',\n",
    "    y='Average_Price',\n",
    "    hue='type',\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "plt.title('Average Price by Vehicle Age and Type (Depreciation Analysis)', fontsize=14)\n",
    "plt.xlabel('Vehicle Age Group')\n",
    "plt.ylabel('Average Price (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.5)\n",
    "plt.legend(title='Vehicle Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Angle 7: Price vs. Odometer Segmentation by Type ---\n",
    "print(\"\\n Final Visualization: Price vs. Odometer Segmentation by Type\")\n",
    "\n",
    "# 1. Filtering (PySpark) - Zaroori types ko filter karna\n",
    "main_types_for_odom = ['sedan', 'truck', 'suv', 'hatchback']\n",
    "df_odom_spark = df_clean.filter(F.col('type').isin(main_types_for_odom))\n",
    "\n",
    "# 2. Sampling (PySpark) - Massive data points ko handle karne ke liye 2% sample lena\n",
    "df_odom_sample = df_odom_spark.select('odometer', 'price', 'type').sample(False, 0.02, seed=42)\n",
    "\n",
    "# 3. PySpark to Pandas Conversion\n",
    "df_odom = df_odom_sample.toPandas()\n",
    "\n",
    "# 4. Visualization (Seaborn)\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Odometer aur Price ka Scatter Plot (Type ke hisaab se)\n",
    "sns.scatterplot(\n",
    "    data=df_odom,\n",
    "    x='odometer',\n",
    "    y='price',\n",
    "    hue='type',\n",
    "    style='type',\n",
    "    palette='deep',\n",
    "    alpha=0.6,\n",
    "    s=30 # Size of points\n",
    ")\n",
    "\n",
    "# X-axis limits ko theek karna (original code ke mutabik)\n",
    "plt.xlim(0, 300000)\n",
    "plt.ylim(0, 50000)\n",
    "\n",
    "plt.title('Price vs. Odometer Reading, Segmented by Vehicle Type', fontsize=16)\n",
    "plt.xlabel('Odometer Reading (Miles)', fontsize=12)\n",
    "plt.ylabel('Price (USD)', fontsize=12)\n",
    "plt.legend(title='Vehicle Type')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Angle 8: Median Price by Title Status ---\n",
    "print(\"\\n Final Visualization: Median Price by Title Status (Risk Devaluation)\")\n",
    "\n",
    "# 1. Filtering & Aggregation (PySpark) - Median Price nikalna\n",
    "main_titles = ['clean', 'salvage', 'rebuilt', 'lien']\n",
    "median_price_data_spark = df_clean.filter(F.col('title_status').isin(main_titles)) \\\n",
    "                                  .groupBy('title_status') \\\n",
    "                                  .agg(F.median('price').alias('Median_Price'))\n",
    "\n",
    "# 2. PySpark to Pandas Conversion\n",
    "median_price_data = median_price_data_spark.toPandas()\n",
    "\n",
    "# 3. Sorting (Pandas) - Price ke hisaab se sort karna\n",
    "median_price_data = median_price_data.sort_values(by='Median_Price', ascending=False)\n",
    "\n",
    "# 4. Visualization (Seaborn)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=median_price_data,\n",
    "    x='title_status',\n",
    "    y='Median_Price',\n",
    "    palette='coolwarm'\n",
    ")\n",
    "\n",
    "plt.title('Median Price by Title Status (Risk Devaluation)', fontsize=16)\n",
    "plt.xlabel('Title Status', fontsize=12)\n",
    "plt.ylabel('Median Price (USD)', fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Analysis: Price vs. Age vs. Mileage Correlation ---\n",
    "\n",
    "print(\"\\n Final Analysis: Which Factor Affects Price More (Age vs. Mileage)?\")\n",
    "\n",
    "# 1. Zaroori numerical columns ko PySpark DataFrame se select karna\n",
    "# Note: 'Age' column pichli visualization mein F.when se ban chuki hai.\n",
    "numerical_features = ['price', 'odometer', 'Age']\n",
    "df_corr_spark = df_clean.select(numerical_features)\n",
    "\n",
    "# 2. PySpark to Pandas conversion for correlation calculation and plotting\n",
    "df_corr = df_corr_spark.toPandas()\n",
    "\n",
    "# 3. Correlation Matrix calculate karna (Pandas ka istemal karke)\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# 4. Heatmap plot karna (Seaborn)\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    linewidths=.5\n",
    ")\n",
    "\n",
    "plt.title('Correlation Heatmap: Which Factor Affects Price Most?', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cec78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"\\n Overall Linear Regression: Depreciation Rate Calculation (PySpark MLlib)\")\n",
    "\n",
    "# 1. Outlier Filtering (PySpark equivalent of Z-score < 3)\n",
    "# Mean aur Standard Deviation calculate karna\n",
    "stats_df = df_clean.agg(\n",
    "    F.mean(\"price\").alias(\"price_mean\"),\n",
    "    F.stddev(\"price\").alias(\"price_std\"),\n",
    "    F.mean(\"odometer\").alias(\"odometer_mean\"),\n",
    "    F.stddev(\"odometer\").alias(\"odometer_std\")\n",
    ").collect()[0]\n",
    "\n",
    "# Filtering Conditions banana (3 standard deviations ke andar)\n",
    "df_depr_filtered = df_clean.filter(\n",
    "    (F.col(\"price\") > stats_df[\"price_mean\"] - 3 * stats_df[\"price_std\"]) &\n",
    "    (F.col(\"price\") < stats_df[\"price_mean\"] + 3 * stats_df[\"price_std\"]) &\n",
    "    (F.col(\"odometer\") > stats_df[\"odometer_mean\"] - 3 * stats_df[\"odometer_std\"]) &\n",
    "    (F.col(\"odometer\") < stats_df[\"odometer_mean\"] + 3 * stats_df[\"odometer_std\"])\n",
    ").select(\"price\", \"odometer\")\n",
    "\n",
    "print(f\"Rows after 3-sigma outlier filtering: {df_depr_filtered.count():,}\")\n",
    "\n",
    "# 2. Vector Assembler: PySpark ML ke liye features ko vector mein jodna\n",
    "assembler = VectorAssembler(inputCols=['odometer'], outputCol=\"features\")\n",
    "df_lr_ready = assembler.transform(df_depr_filtered)\n",
    "\n",
    "# 3. Distributed Linear Regression Model Fit Karna (PySpark MLlib)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr_model = lr.fit(df_lr_ready)\n",
    "\n",
    "# 4. Result Nikalna: Coefficient (Depreciation Rate)\n",
    "depreciation_per_mile = lr_model.coefficients[0]\n",
    "loss_per_10k_miles = depreciation_per_mile * 10000\n",
    "\n",
    "# Final Results Print Karna\n",
    "print(f\"\\n Odometer Coefficient (Depreciation Rate per 1 Mile): ${depreciation_per_mile:,.4f}\")\n",
    "print(f\" Loss Per 10,000 Miles: ${loss_per_10k_miles:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n Visualization: Price vs. Odometer Regression Trend (Final Visual)\")\n",
    "\n",
    "# 1. Outlier Filtering (PySpark Z-score filtering logic)\n",
    "# PySpark mein Z-score filtering ke liye Mean aur Standard Deviation dobara calculate karna\n",
    "stats_df = df_clean.agg(\n",
    "    F.mean(\"price\").alias(\"price_mean\"),\n",
    "    F.stddev(\"price\").alias(\"price_std\"),\n",
    "    F.mean(\"odometer\").alias(\"odometer_mean\"),\n",
    "    F.stddev(\"odometer\").alias(\"odometer_std\")\n",
    ").collect()[0]\n",
    "\n",
    "# Filtering Conditions banana (3 standard deviations ke andar)\n",
    "df_depr_filtered_spark = df_clean.filter(\n",
    "    (F.col(\"price\") > stats_df[\"price_mean\"] - 3 * stats_df[\"price_std\"]) &\n",
    "    (F.col(\"price\") < stats_df[\"price_mean\"] + 3 * stats_df[\"price_std\"]) &\n",
    "    (F.col(\"odometer\") > stats_df[\"odometer_mean\"] - 3 * stats_df[\"odometer_std\"]) &\n",
    "    (F.col(\"odometer\") < stats_df[\"odometer_mean\"] + 3 * stats_df[\"odometer_std\"])\n",
    ").select(\"price\", \"odometer\")\n",
    "\n",
    "# 2. Sampling (PySpark) - Plotting ke liye 2% sample lena\n",
    "df_depr_sample = df_depr_filtered_spark.sample(False, 0.02, seed=42)\n",
    "\n",
    "# 3. PySpark to Pandas Conversion\n",
    "df_depr = df_depr_sample.toPandas()\n",
    "\n",
    "# 4. Visualization (Seaborn regplot)\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# 'regplot' automatically scatter plot banata hai aur uspar linear trend line fit karta hai.\n",
    "sns.regplot(\n",
    "    data=df_depr,\n",
    "    x='odometer',\n",
    "    y='price',\n",
    "    scatter_kws={'alpha': 0.3, 's': 10}, # Points ko halka aur chota karna\n",
    "    line_kws={'color': 'red', 'lw': 3}   # Trend line ko mota aur laal karna\n",
    ")\n",
    "\n",
    "# Limits set karna (taaki plot zyada clear ho)\n",
    "plt.xlim(0, 300000)\n",
    "plt.ylim(0, 50000)\n",
    "\n",
    "plt.title('Car Price Depreciation vs. Odometer Reading (Mileage)', fontsize=16)\n",
    "plt.xlabel('Odometer Reading (Miles)', fontsize=12)\n",
    "plt.ylabel('Price (USD)', fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8787f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "print(\"\\n Starting Comprehensive Distributed Analysis (Master Metrics & Grouped Regression)\")\n",
    "\n",
    "# ====================================================================\n",
    "# PART A: OVERALL KPI CALCULATION (PySpark MLlib & Save)\n",
    "# ====================================================================\n",
    "\n",
    "# 1. PySpark MLlib: Linear Regression (Z-score filtering ke baad)\n",
    "# (Assumes df_clean already has the data after all previous steps)\n",
    "\n",
    "# Filtering (3-sigma Outlier Removal)\n",
    "stats_df = df_clean.agg(F.mean(\"price\").alias(\"price_mean\"),\n",
    "                        F.stddev(\"price\").alias(\"price_std\"),\n",
    "                        F.mean(\"odometer\").alias(\"odometer_mean\"),\n",
    "                        F.stddev(\"odometer\").alias(\"odometer_std\")).collect()[0]\n",
    "\n",
    "df_depr_filtered = df_clean.filter(\n",
    "    (F.col(\"price\") > stats_df[\"price_mean\"] - 3 * stats_df[\"price_std\"]) &\n",
    "    (F.col(\"price\") < stats_df[\"price_mean\"] + 3 * stats_df[\"price_std\"]) &\n",
    "    (F.col(\"odometer\") > stats_df[\"odometer_mean\"] - 3 * stats_df[\"odometer_std\"]) &\n",
    "    (F.col(\"odometer\") < stats_df[\"odometer_mean\"] + 3 * stats_df[\"odometer_std\"])\n",
    ").select(\"price\", \"odometer\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['odometer'], outputCol=\"features\")\n",
    "df_lr_ready = assembler.transform(df_depr_filtered)\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr_model = lr.fit(df_lr_ready)\n",
    "depreciation_per_mile = lr_model.coefficients[0]\n",
    "loss_per_10k_miles = depreciation_per_mile * 10000\n",
    "\n",
    "# 2. Key Metric DataFrame Banana aur Save Karna\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Overall Depreciation per 1 Mile', 'Overall Loss Per 10k Miles'],\n",
    "    'Value': [depreciation_per_mile, loss_per_10k_miles],\n",
    "    'Unit': ['USD', 'USD']\n",
    "})\n",
    "\n",
    "results_df.to_csv('reports/analysis_key_metrics.csv', index=False)\n",
    "print(\" analysis_key_metrics.csv file saved (Overall KPI).\")\n",
    "\n",
    "\n",
    "# ====================================================================\n",
    "# PART B: GROUPED REGRESSION (MASTER DEPRECIATION TABLE)\n",
    "# ====================================================================\n",
    "\n",
    "# 1. Pandas UDF Schema Definition\n",
    "schema = StructType([\n",
    "    StructField(\"manufacturer\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"Model_Count\", LongType(), True),\n",
    "    StructField(\"Depreciation_per_Mile\", DoubleType(), True),\n",
    "    StructField(\"Loss_Per_10k_Miles\", DoubleType(), True),\n",
    "    StructField(\"R2_Score\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# 2. Pandas UDF Definition (OLS Model Run per Group)\n",
    "@F.pandas_udf(schema, functionType=F.PandasUDFType.GROUPED_MAP)\n",
    "def calculate_depreciation_udf(pandas_df):\n",
    "    \"\"\"Har (manufacturer, model) group ke liye OLS regression chalaana.\"\"\"\n",
    "    # Kam se kam 50 listings zaroori hain reliable calculation ke liye (jaisa ki aapne manga)\n",
    "    if len(pandas_df) < 50:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        X = sm.add_constant(pandas_df['odometer'])\n",
    "        y = pandas_df['price']\n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "        depreciation_rate = model.params['odometer']\n",
    "\n",
    "        # Result Pandas Series banana\n",
    "        result = pd.Series({\n",
    "            'manufacturer': pandas_df['manufacturer'].iloc[0],\n",
    "            'model': pandas_df['model'].iloc[0],\n",
    "            'Model_Count': len(pandas_df),\n",
    "            'Depreciation_per_Mile': depreciation_rate,\n",
    "            'Loss_Per_10k_Miles': depreciation_rate * 10000,\n",
    "            'R2_Score': model.rsquared\n",
    "        })\n",
    "        return result.to_frame().T\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# 3. Grouped Map Apply Karna (Distributed Analysis)\n",
    "master_depreciation_results_spark = df_clean.groupby(\"manufacturer\", \"model\").apply(calculate_depreciation_udf)\n",
    "\n",
    "# 4. Final Cleanup aur Save Karna (Master Table)\n",
    "# Depreciation Rate ko positive loss mein badalna\n",
    "master_depreciation_results_spark = master_depreciation_results_spark.withColumn(\n",
    "    'Est_Loss_Per_10k_Miles_USD', F.col('Loss_Per_10k_Miles') * -1)\n",
    "master_depreciation_results_spark = master_depreciation_results_spark.withColumn(\n",
    "    'Model_Reliability_R2', F.round(F.col('R2_Score'), 3))\n",
    "\n",
    "\n",
    "# Final columns chunna aur save karna (jaisa ki aapke original code mein tha)\n",
    "df_final_master = master_depreciation_results_spark.select(\n",
    "    \"manufacturer\",\n",
    "    \"model\",\n",
    "    \"Model_Count\",\n",
    "    \"Est_Loss_Per_10k_Miles_USD\",\n",
    "    \"Model_Reliability_R2\"\n",
    ")\n",
    "\n",
    "df_final_master.coalesce(1).write.csv(\n",
    "    \"reports/MASTER_DEPRECIATION_RATES.csv\",\n",
    "    mode=\"overwrite\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\" MASTER_DEPRECIATION_RATES.csv file successfully created (Power BI Master Table).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n Final Visualization: Top 10 Models by Lowest Depreciation Loss\")\n",
    "\n",
    "# 1. Master Depreciation File Load Karna (Pandas mein, kyunki yeh chota final metrics file hai)\n",
    "df_master_depr = pd.read_csv('/content/reports/MASTER_DEPRECIATION_RATES.csv/part-00000-e1d4306c-8b4d-48b8-97ab-a11a05dd1b7e-c000.csv')\n",
    "\n",
    "# 2. Results ko Filter aur Sort Karna\n",
    "# Acche results ke liye, sirf woh models chunein jinki reliability (R2) 0.5 se zyada hai.\n",
    "df_reliable_depr = df_master_depr[df_master_depr['Model_Reliability_R2'] > 0.5]\n",
    "\n",
    "# Loss sabse kam ho (Lowest Loss = Best Value Retention)\n",
    "df_top_10_low_loss = df_reliable_depr.sort_values(\n",
    "    by='Est_Loss_Per_10k_Miles_USD',\n",
    "    ascending=True\n",
    ").head(10)\n",
    "\n",
    "# 'manufacturer' aur 'model' ko jodkar label banana\n",
    "df_top_10_low_loss['Model_Label'] = df_top_10_low_loss['manufacturer'] + ' ' + df_top_10_low_loss['model']\n",
    "\n",
    "# 3. Visualization (Bar Plot)\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "sns.barplot(\n",
    "    data=df_top_10_low_loss,\n",
    "    x='Est_Loss_Per_10k_Miles_USD',\n",
    "    y='Model_Label',\n",
    "    palette='viridis_r' # Reverse palette taaki \"Best\" models (Lowest Loss) dark green dikhein\n",
    ")\n",
    "\n",
    "plt.title('Top 10 Vehicles by Lowest Estimated Loss per 10k Miles (R-squared > 0.5)', fontsize=14)\n",
    "plt.xlabel('Estimated Loss Per 10,000 Miles (USD)')\n",
    "plt.ylabel('Vehicle Model')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad4a404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
